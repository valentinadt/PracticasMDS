col = "darkblue", lwd = 2, add = TRUE, yaxt = "n")
hist(mydata$cc, density = 20, breaks = 10, prob = TRUE,
xlab = "Cilindrada",
main = "Distribución de la cilindrada")
curve(dnorm(x, mean = mean(mydata$cc),
sd = sqrt(var(mydata$cc))),
col = "darkblue", lwd = 2, add = TRUE, yaxt = "n")
hist(mydata$rpm, density = 20, breaks = 10, prob = TRUE,
xlab = "Revoluciones",
main = "Distribución de las revoluciones")
curve(dnorm(x, mean = mean(mydata$rpm),
sd = sqrt(var(mydata$rpm))),
col = "darkblue", lwd = 2, add = TRUE, yaxt = "n")
View(mydata)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
View(mydata)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
unique(mydata$marca)
cor(mydata)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
library(corrplot)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
unique(mydata$marca)
pvp_Euros <- mydata$pvp /166.386 #1 euro = 166.386 pesetas
mydata['pvpEuros'] = pvp_Euros
#eliminación de las columnas
mydata <- select(mydata, -acelerac,-marca, -modelo,-pvp )
#sustitución de NA´s por la media en cons120
mydata[is.na(mydata$cons120), "cons120"] <- mean(mydata$cons120, na.rm=T)
#sustitución de NA´s por la media en cons90
mydata[is.na(mydata$cons90), "cons90"] <- mean(mydata$cons90, na.rm=T)
#sustitución de NA´s por la media en peso
mydata[is.na(mydata$peso), "peso"] <- mean(mydata$peso, na.rm=T)
#sustitución de NA´s por la media en consurb
mydata[is.na(mydata$consurb), "consurb"] <- mean(mydata$consurb, na.rm=T)
#sustitución de NA´s por la media en velocida
mydata[is.na(mydata$velocida), "velocida"] <- mean(mydata$velocida, na.rm=T)
cor(mydata)
rcorr(as.matrix(mydata))
correlacion<-round(cor(mydata), 1)
cor(mydata)
rcorr(as.matrix(mydata))
correlacion<-round(cor(mydata), 1)
plot(correlacion)
corrplot(correlacion)
knitr::opts_chunk$set(echo = TRUE)
library("quantmod") #Package to download financials historical data
library(forecast)
library("fGarch")
library(vars)
#funciones
archTest <- function(rtn,m=10){
y=(rtn-mean(rtn))^2
T=length(rtn)
atsq=y[(m+1):T]
x=matrix(0,(T-m),m)
for (i in 1:m){
x[,i]=y[(m+1-i):(T-i)]
}
md=lm(atsq~x)
summary(md)
}
#Carga de datos de BMW y Volkswagen
BSymbol="BMW.DE"
VSymbol="VOW3.DE"
#seleccionar los datos desde 2014 hasta hoy
mData<-getSymbols(BSymbol ,from="2014-01-01",to="2020-11-26",auto.assign=FALSE)
mData1<-getSymbols(VSymbol ,from="2014-01-01",to="2020-11-26",auto.assign=FALSE)
#Definir datos
xData=Ad(mData)
xData1=Ad(mData1)
#Calcular la rentabilidad diaria
dRentCont=dailyReturn(xData,type='log',leading=FALSE)
dRentCont1=dailyReturn(xData1,type='log',leading=FALSE)
#Excluir NA's
dRentCont=na.exclude(dRentCont)
dRentCont=na.exclude(dRentCont)
View(mData)
mData3 <- merge(mData,mData1)
View(mData3)
#Definir datos
etiqueta <- mData3.columns
etiqueta
#Definir datos
etiqueta <- col(mData3)
etiqueta
#Definir datos
etiqueta <- names(mData3)
etiqueta
etiqueta <- filter(names(mData3) == "BMW*")
filter(etiquetas == "BMW*")
filter(etiqueta == "BMW*")
#Definir datos
xData=Ad(mData3)
View(xData)
#Definir datos
xData=Ad(mData3)
View(xData)
xData=Ad(mData)
xData1=Ad(mData1)
View(xData)
xData3 = Ad(mData3)
View(xData3)
View(mData3)
mData3 <- merge(mData,mData1)
xData3 = Ad(mData3)
View(xData3)
knitr::opts_chunk$set(echo = TRUE)
#ARCH(1)
m1=garchFit(~1+garch(1,0),data=dRentCont,trace=F) # Fit an ARCH(1) model
knitr::opts_chunk$set(echo = TRUE)
library("quantmod") #Package to download financials historical data
library(forecast)
library("fGarch")
library(vars)
#funciones
archTest <- function(rtn,m=10){
y=(rtn-mean(rtn))^2
T=length(rtn)
atsq=y[(m+1):T]
x=matrix(0,(T-m),m)
for (i in 1:m){
x[,i]=y[(m+1-i):(T-i)]
}
md=lm(atsq~x)
summary(md)
}
#Carga de datos de BMW y Volkswagen
BSymbol="BMW.DE"
VSymbol="VOW3.DE"
#seleccionar los datos desde 2014 hasta hoy
mData<-getSymbols(BSymbol ,from="2014-01-01",to="2020-11-26",auto.assign=FALSE)
mData1<-getSymbols(VSymbol ,from="2014-01-01",to="2020-11-26",auto.assign=FALSE)
#Definir datos
xData=Ad(mData)
xData1=Ad(mData1)
#Calcular la rentabilidad diaria
dRentCont=dailyReturn(xData,type='log',leading=FALSE)
dRentCont1=dailyReturn(xData1,type='log',leading=FALSE)
#Excluir NA's
dRentCont=na.exclude(dRentCont)
dRentCont=na.exclude(dRentCont)
#plot de zoo de los datos de BMW
plot.zoo(cbind(xData,dRentCont),main=paste(BSymbol," y  Rentabilidad"),xlab="años",ylab=c("Precio","rentabilidad"))
grid(lwd=2)
#Plot return squared
plot.zoo(cbind(Ad(mData),dRentCont,dRentCont^2),main=paste(BSymbol," y  Rentabilidad"),xlab="años",ylab=c("Precio","rentabilidad","Volatilidad"))
t.test(dRentCont)
VolProxy=dRentCont^2 #al cuadrado para conocer la volatilidad
#gráfico
tsdisplay(VolProxy)
#ARCH(1)
m1=garchFit(~1+garch(1,0),data=dRentCont,trace=F) # Fit an ARCH(1) model
resi=residuals(m1,standardize=T) #residuals
resi=xts(resi,order.by=index(dRentCont)) #residuals as xts
tsdisplay(resi^2) #acf pacf residuals
summary(m1)
#GARCH(1,1)
m2=garchFit(~1+garch(1,1),data=dRentCont,trace=F) # Fit an GARCH(1,1) model
summary(m2)
resi=residuals(m2,standardize=T) #residuals
resi=xts(resi,order.by=index(dRentCont)) #residuals as xts
tsdisplay(resi^2) #acf pacf residuals
plot(m2, which = 1)
#t-student
m3=garchFit(~1+garch(1,1),data=dRentCont,trace=F,cond.dist="std")
summary(m3)
plot(m3 , which = 1)
v1=volatility(m3)  #volatilidad
v1=xts(v1,order.by=index(dRentCont)) #volatilidad con xts0
plot(sqrt(252)*v1)
residuos=residuals(m3,standardize=T) # Standardized residuals
resi=xts(residuos,order.by=index(dRentCont)) # Standardized residuals as XTS
tsdisplay(residuos^2) #acf pacf residuals
plot(residuos)
predict(m3) #volatilidad forecast
predict(m3, n.ahead = 10, plot=TRUE, crit_val=2) #plot con 2*standard error
predict(m3,n.ahead=20,plot=TRUE,conf=.9,nx=100) # plot con 100 datos y con 90% de nivel de confianza
## Leer datos
bmw=getSymbols("BMW.DE",env=NULL)
vow=getSymbols("VOW3.DE",env=NULL)
# Generar rentabilidad mensual
rbmw=monthlyReturn(bmw[,6])
rvow=monthlyReturn(vow[,6])
#generar un vector
vY=cbind(rbmw,rvow)
colnames(vY)=c("BMW","VOW")
vY=na.omit(vY)
#Selección del modelo VAR
VARselect(vY)
#Estimación
model.var=VAR(vY)
summary(model.var)
model.var1=VAR(vY,type="none")
summary(model.var1)
causality(model.var1)
#respuesta al impulso
model.ri=irf(model.var1)
model.ri
plot(model.ri)
#prediccion
predict(model.var1, n.ahead = 8, ci = 0.95)
setwd("C:/Users/ajrd9/Desktop/R")
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
library(corrplot)
library(cluster)
library(factoextra)
mydata <- read_sav("tterreno.sav")
setwd("C:/Users/ajrd9/Desktop/R")
mydata <- read_sav("tterreno.sav")
mydata <- read_sav("/tterreno.sav")
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
library(corrplot)
library(cluster)
library(factoextra)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
mydata <- read_sav("tterreno.sav")
knitr::opts_chunk$set(echo = TRUE)
mydata <- read_sav("tterreno.sav")
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
unique(mydata$marca)
pvp_Euros <- mydata$pvp /166.386 #1 euro = 166.386 pesetas
mydata['pvpEuros'] = pvp_Euros
#eliminación de las columnas
mydata <- select(mydata, -acelerac,-marca, -modelo,-pvp )
#sustitución de NA´s por la media en cons120
mydata[is.na(mydata$cons120), "cons120"] <- mean(mydata$cons120, na.rm=T)
#sustitución de NA´s por la media en cons90
mydata[is.na(mydata$cons90), "cons90"] <- mean(mydata$cons90, na.rm=T)
#sustitución de NA´s por la media en peso
mydata[is.na(mydata$peso), "peso"] <- mean(mydata$peso, na.rm=T)
#sustitución de NA´s por la media en consurb
mydata[is.na(mydata$consurb), "consurb"] <- mean(mydata$consurb, na.rm=T)
#sustitución de NA´s por la media en velocida
mydata[is.na(mydata$velocida), "velocida"] <- mean(mydata$velocida, na.rm=T)
cor(mydata)
rcorr(as.matrix(mydata))
correlacion<-round(cor(mydata), 1)
mydata <- select(mydata, -potencia, -acel2, -cons120, -consurb )
mydata <- select(mydata, -cilindro )
colnames(mydata)
par(mfrow=c(1,1))
boxplot(mydata$pvpEuros,main = "Precio Vehiculos",
ylab = "Precio (Euros)", notch = FALSE)
par(mfrow=c(1,1))
boxplot(mydata$velocida,main = "Velocidad Vehículos",
ylab = "Velocidad (Km/h)", notch = FALSE)
lbls1 <- c("2 plazas", "4 plazas", "5 plazas", "6 plazas", "7 plazas", "8 plazas", "9 plazas")
color <- c("blue", "aquamarine", "orange", "purple4","black", "salmon","white")
par(mfrow=c(1,1))
barplot(table(as.factor(mydata$velocida)), main="Velocidad Máxima del Vehículo", xlab = "Velocidad (Km/h)", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$cons90)), main="Consumo Medio a 90 km/h" , xlab = "Consumo (km/h)", ylab = "Nº Coches", col=palette())
pie(x = table(mydata$plazas), labels = lbls1, main="Plazas del vehículo", col=palette())
q.dist =get_dist(mydata, stand = TRUE, method = "pearson")
fviz_dist(q.dist, gradient =list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
round(as.matrix(dist.eucl)[1:6, 1:6], 1)
round(as.matrix(dist.eucl)[1:6, 1:6], 1)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
library(corrplot)
library(cluster)
library(factoextra)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
unique(mydata$marca)
pvp_Euros <- mydata$pvp /166.386 #1 euro = 166.386 pesetas
mydata['pvpEuros'] = pvp_Euros
#eliminación de las columnas
mydata <- select(mydata, -acelerac,-marca, -modelo,-pvp )
#sustitución de NA´s por la media en cons120
mydata[is.na(mydata$cons120), "cons120"] <- mean(mydata$cons120, na.rm=T)
#sustitución de NA´s por la media en cons90
mydata[is.na(mydata$cons90), "cons90"] <- mean(mydata$cons90, na.rm=T)
#sustitución de NA´s por la media en peso
mydata[is.na(mydata$peso), "peso"] <- mean(mydata$peso, na.rm=T)
#sustitución de NA´s por la media en consurb
mydata[is.na(mydata$consurb), "consurb"] <- mean(mydata$consurb, na.rm=T)
#sustitución de NA´s por la media en velocida
mydata[is.na(mydata$velocida), "velocida"] <- mean(mydata$velocida, na.rm=T)
cor(mydata)
rcorr(as.matrix(mydata))
correlacion<-round(cor(mydata), 1)
mydata <- select(mydata, -potencia, -acel2, -cons120, -consurb )
mydata <- select(mydata, -cilindro )
colnames(mydata)
par(mfrow=c(1,1))
boxplot(mydata$pvpEuros,main = "Precio Vehiculos",
ylab = "Precio (Euros)", notch = FALSE)
par(mfrow=c(1,1))
boxplot(mydata$velocida,main = "Velocidad Vehículos",
ylab = "Velocidad (Km/h)", notch = FALSE)
lbls1 <- c("2 plazas", "4 plazas", "5 plazas", "6 plazas", "7 plazas", "8 plazas", "9 plazas")
color <- c("blue", "aquamarine", "orange", "purple4","black", "salmon","white")
par(mfrow=c(1,1))
barplot(table(as.factor(mydata$velocida)), main="Velocidad Máxima del Vehículo", xlab = "Velocidad (Km/h)", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$cons90)), main="Consumo Medio a 90 km/h" , xlab = "Consumo (km/h)", ylab = "Nº Coches", col=palette())
pie(x = table(mydata$plazas), labels = lbls1, main="Plazas del vehículo", col=palette())
q.dist =get_dist(mydata, stand = TRUE, method = "pearson")
fviz_dist(q.dist, gradient =list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
round(as.matrix(dist.eucl)[1:6, 1:6], 1)
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
library(corrplot)
library(factoextra)
library(cluster)
library("NbClust")
#Carga de datos
cochesJefe <- read_sav("tterreno.sav")
cochesJefe <- tbl_df(cochesJefe)
#creación de una columna con el precio en euros
pvp_Euros <- cochesJefe$pvp /166.386 #1 euro = 166.386 pesetas
cochesJefe['pvpEuros'] = pvp_Euros
#quedarnos con las variables con las que vamos a trabajar
cochesJefe <- select(cochesJefe, -potencia, -acel2, -cons120, -consurb,-cilindro, -acelerac,-marca, -modelo,-pvp)
#sustitución de los valores NA's
cochesJefe=na.omit(cochesJefe)
# Escalar los datos para que sea más fácil trabajar con ellos.
cochesJefe.esc =scale(cochesJefe, center=TRUE, scale = TRUE)
set.seed(123)
prueba1 =kmeans(cochesJefe.esc, 3)
fviz_cluster(list(data = cochesJefe.esc, cluster = prueba1$cluster),
frame.type = "norm", geom = "point", stand = FALSE)
q.dist =get_dist(cochesJefe.esc, stand = TRUE, method = "pearson")
fviz_dist(q.dist, gradient =list(low = "#00AFBB", mid = "white", high = "#FC4E07"), lab_size = 5)
dist.eucl <- dist(cochesJefe.esc, method = "euclidean")
round(as.matrix(dist.eucl)[1:7, 1:7], 1)
cochesJefe.cor=cor(t(cochesJefe.esc[,-1]),  method = "pearson")# Empleamos quiebras, que es el objeto re-escalado, sin la primera columna.
round(cochesJefe.cor[1:6, 1:6], 2)
dist.cor=as.dist(1 - cochesJefe.cor)# Las correlaciones negativas tendrán en la nueva matriz de distancias un valor > 1
round(as.matrix(dist.cor)[1:6, 1:6], 2)
corrplot(as.matrix(dist.eucl), is.corr = FALSE, method = "color", type="lower",diag=F, order="hclust", tl.cex=0.6, tl.col="blue")
fviz_dend(hclust(dist(cochesJefe.esc)), k = 4,  cex = 0.8, main = "Dendrograma Coches Jefe")
plot(hclust(dist.eucl, method = "ward.D2"), cex=0.6, main="Dendrograma", ylab="Anchura",xlab="Análisis cluster aplicando Ward sobre matriz de distancias euclídeas")
heatmap(as.matrix(dist.eucl), symm = TRUE, distfun = function(x)as.dist(x))
cochesJefe.eclust =eclust(cochesJefe.esc, FUNcluster = "kmeans", stand=TRUE,hc_metric="euclidean", nstart=25)
# podemos fijar el número de clusters añadiendo k=número de clusters, por ejemplo k=3
cochesJefe.eclust =eclust(cochesJefe.esc, FUNcluster = "kmeans", stand=TRUE,hc_metric="euclidean", nstart=25, k=4)
fviz_silhouette(cochesJefe.eclust)
cochesJefe.eclust$nbclust
set.seed(123)
k.max = 15# Máximo número de clusters
# lo aplicamos sobre sec.def, ya tipificado
data=cochesJefe.esc
wss=sapply(1:k.max,
function(k){kmeans(data, k, nstart=10 )$tot.withinss})
plot(1:k.max, wss,
type="b", pch = 19, frame = FALSE,
xlab="Número K de clusters",
ylab="Suma total de cuadrados intra-clusters")
abline(v = 3, lty =2)
abline(v = 4, lty =3)
fviz_nbclust(data, kmeans, method = "wss") +geom_vline(xintercept = 3, linetype = 2) +geom_vline(xintercept = 4, linetype = 3) +ggtitle("Número óptimo de clusters - k medias") +labs(x="Número k de clusters",y="Suma total de cuadrados intra grupos")
# Sugiere más 2 ó 4 grupos que 3
fviz_nbclust(data,  cluster::pam, method = "wss") +
geom_vline(xintercept = 2, linetype = 2) +
geom_vline(xintercept = 4, linetype = 3) +ggtitle("Número óptimo de clusters - PAM") +
labs(x="Número k de clusters",y="Suma total de cuadrados intra grupos")
# Sugiere más bien dos gruposfviz_nbclust(data,  hcut, method = "wss") +geom_vline(xintercept = 2, linetype = 2) +ggtitle("Número óptimo de clusters - jerárquico") +labs(x="Número k de clusters",y="Suma total de cuadrados intra grupos")
k.max = 10
# cálculo del perfil promedio para número de clusters entre k = 2 y k = 15
sil =rep(0, k.max)
#Bucle de cálculo sobre k-medias
for(i in 2:k.max){
km.data =kmeans(data, centers = i, nstart = 25)
ss =silhouette(km.data$cluster,dist(data))
sil[i] =mean(ss[, 3])
}
# Gráfico de la anchura del perfil
plot(1:k.max, sil, type = "b", pch = 19,
frame = FALSE, xlab = "Número k de clusters")
abline(v =which.max(sil), lty = 2)
set.seed(123)
clus.nb =NbClust(data, distance = "euclidean",
min.nc = 2, max.nc = 10,
method = "complete",index ="gap")
clus.nb# resultados
nb.todos =NbClust(data, distance = "euclidean", min.nc = 2,max.nc = 10, method = "complete", index ="all")
fviz_nbclust(nb.todos) +theme_minimal() +labs(x="Número k de clusters", y="Frecuencia")
---
title: "Los coches del jefe, parte 2. Cómo los reparto. "
author: "Valentina Díaz Torres"
date: "01/12/2020"
output: pdf_document
---
setwd("C:/Users/ajrd9/Desktop/petalditaQuerida/PrankAgrupa")
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(ggplot2)
library(readxl)
library(gmodels)
library(Hmisc)
library(ggthemes)
library(fastDummies)
library(car)
library(corrplot)
library(mctest)
library(tidyverse)
library(skimr)
mydata <- read_sav("tterreno.sav")
mydata <- tbl_df(mydata)
summary(mydata) #Existen 46 NA´s. También se obtienen otros valores relevante como el máximo, mínimo y media de cada una de las variables.
#eliminación de la columna acelerac
mydata <- select(mydata, -acelerac)
#sustitución de NA´s por la media en cons120
mydata[is.na(mydata$cons120), "cons120"] <- mean(mydata$cons120, na.rm=T)
#sustitución de NA´s por la media en cons90
mydata[is.na(mydata$cons90), "cons90"] <- mean(mydata$cons90, na.rm=T)
#sustitución de NA´s por la media en peso
mydata[is.na(mydata$peso), "peso"] <- mean(mydata$peso, na.rm=T)
#sustitución de NA´s por la media en consurb
mydata[is.na(mydata$consurb), "consurb"] <- mean(mydata$consurb, na.rm=T)
#sustitución de NA´s por la media en velocida
mydata[is.na(mydata$velocida), "velocida"] <- mean(mydata$velocida, na.rm=T)
pvp_Euros <- mydata$pvp /166.386 #1 euro = 166.386 pesetas
mydata['pvpEuros'] = pvp_Euros
glimpse(mydata)
#attributes(mydata$marca)
unique(mydata$marca)
mydata$modelo<-as.factor(mydata$modelo)
par(mfrow=c(1,1))
boxplot(mydata$pvpEuros,main = "Precio Vehiculos",
ylab = "Precio (Euros)", notch = FALSE)
par(mfrow=c(1,1))
boxplot(mydata$potencia,main = "Potencia Vehículos",
ylab = "Potencia (CV)", notch = FALSE)
par(mfrow=c(1,1))
boxplot(mydata$velocida,main = "Velocidad Vehículos",
ylab = "Velocidad (Km/h)", notch = FALSE)
lbls <- c("4 cilindros", "6 cilindros","8 cilindros")
lbls1 <- c("2 plazas", "4 plazas", "5 plazas", "6 plazas", "7 plazas", "8 plazas", "9 plazas")
color <- c("blue", "aquamarine", "orange", "purple4","black", "salmon","white")
par(mfrow=c(1,1))
barplot(table(as.factor(mydata$cilindro)), main="Cilindros", xlab = "Nº de Cilindros", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$velocida)), main="Velocidad Máxima del Vehículo", xlab = "Velocidad (Km/h)", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$cons120)), main="Consumo Medio a 120 km/h" , xlab = "Consumo (km/h)", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$plazas)), main="Plazas del vehículo" , xlab = "Nº de plazas", ylab = "Nº Coches", col=palette())
barplot(table(as.factor(mydata$marca)),main="Número de marcas", xlab = "Nº de marcas", ylab = "Nº Coches", col=palette())
numCB <- length(bajoCons$cons120)
